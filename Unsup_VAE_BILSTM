# tfidf_seqvae_pipeline.py
"""
TF-IDF + Sequence-to-Vector VAE Pipeline (OOP)

Features:
1. Train-only TF-IDF fit
2. Continuous sequence selection via sliding window
3. SeqVAE encoding / decoding
4. Supervised classification on latent space
5. Train + Test accuracy
6. Single-text prediction API

Author: ready-to-run
"""

import re
from collections import Counter
from typing import List, Optional, Dict, Any

import numpy as np
import pandas as pd

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader

# =========================
# Text utilities
# =========================

def keep_english(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


# =========================
# TF-IDF window selector
# =========================

class TfidfWindowSelector:
    def __init__(self, window_size: int = 50, stride: int = 1):
        self.window_size = window_size
        self.stride = stride
        self.vectorizer: Optional[TfidfVectorizer] = None

    def fit(self, texts: List[str]):
        self.vectorizer = TfidfVectorizer(
            ngram_range=(1, 3),
            min_df=2,
            max_df=0.9
        )
        self.vectorizer.fit(texts)
        return self

    def best_window(self, text: str) -> str:
        tokens = text.split()
        if len(tokens) <= self.window_size:
            return text

        best_score = -1.0
        best_text = text

        for i in range(0, len(tokens) - self.window_size + 1, self.stride):
            win = tokens[i:i + self.window_size]
            win_text = " ".join(win)
            vec = self.vectorizer.transform([win_text])
            score = vec.sum()

            if score > best_score:
                best_score = score
                best_text = win_text

        return best_text


# =========================
# Tokenizer
# =========================

class SimpleTokenizer:
    def __init__(self, max_vocab: int = 10000, min_freq: int = 2):
        self.max_vocab = max_vocab
        self.min_freq = min_freq
        self.word2idx = {"<PAD>": 0, "<OOV>": 1}
        self.idx2word = {0: "<PAD>", 1: "<OOV>"}

    def fit(self, texts: List[str]):
        cnt = Counter()
        for t in texts:
            cnt.update(t.split())

        words = [w for w, f in cnt.items() if f >= self.min_freq]
        words = sorted(words, key=lambda w: -cnt[w])[: self.max_vocab]

        for w in words:
            if w not in self.word2idx:
                idx = len(self.word2idx)
                self.word2idx[w] = idx
                self.idx2word[idx] = w

    @property
    def vocab_size(self):
        return len(self.word2idx)

    def texts_to_sequences(self, texts: List[str]) -> List[List[int]]:
        return [[self.word2idx.get(w, 1) for w in t.split()] for t in texts]


def pad_sequences(seqs: List[List[int]], max_len: int):
    arr = np.zeros((len(seqs), max_len), dtype=np.int64)
    lengths = np.zeros(len(seqs), dtype=np.int64)

    for i, s in enumerate(seqs):
        l = min(len(s), max_len)
        arr[i, :l] = s[:l]
        lengths[i] = l

    return arr, lengths


# =========================
# SeqVAE Model
# =========================

class SeqVAE(nn.Module):
    def __init__(self, vocab_size: int, seq_len: int,
                 emb_dim: int = 6, hidden_dim: int = 128, latent_dim: int = 24):
        super().__init__()
        self.seq_len = seq_len
        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)

        self.encoder = nn.LSTM(
            emb_dim, hidden_dim, batch_first=True, bidirectional=True
        )

        self.fc_mu = nn.Linear(hidden_dim * 2, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim * 2, latent_dim)

        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, seq_len * emb_dim)
        )

    def encode(self, x, lengths):
        emb = self.emb(x)
        packed = nn.utils.rnn.pack_padded_sequence(
            emb, lengths.cpu(), batch_first=True, enforce_sorted=False
        )
        _, (h, _) = self.encoder(packed)
        h = torch.cat([h[-2], h[-1]], dim=1)
        return self.fc_mu(h), self.fc_logvar(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        out = self.decoder(z)
        return out.view(-1, self.seq_len, self.emb.embedding_dim)

    def forward(self, x, lengths):
        mu, logvar = self.encode(x, lengths)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar


# =========================
# Dataset
# =========================

class SeqDataset(Dataset):
    def __init__(self, ids, lengths):
        self.ids = torch.tensor(ids)
        self.lengths = torch.tensor(lengths)

    def __len__(self):
        return len(self.ids)

    def __getitem__(self, idx):
        return self.ids[idx], self.lengths[idx]


# =========================
# High-level Pipeline
# =========================

class TextSeqVAEPipeline:
    def __init__(self, df: pd.DataFrame, text_col: str, label_col: str,
                 max_seq_len: int = 50, device: Optional[str] = None):

        self.df = df.copy()
        self.text_col = text_col
        self.label_col = label_col
        self.max_seq_len = max_seq_len
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        self.df["_clean"] = self.df[text_col].apply(keep_english)

        self.tfidf = TfidfWindowSelector(window_size=max_seq_len)
        self.tokenizer = SimpleTokenizer()
        self.model: Optional[SeqVAE] = None
        self.classifier: Optional[LogisticRegression] = None

    def split(self, test_size=0.2, random_state=42):
        self.train_df, self.test_df = train_test_split(
            self.df, test_size=test_size, random_state=random_state
        )

    def prepare(self):
        self.tfidf.fit(self.train_df["_clean"].tolist())

        for dfp in [self.train_df, self.test_df]:
            dfp["_chunk"] = dfp["_clean"].apply(self.tfidf.best_window)

        self.tokenizer.fit(self.train_df["_chunk"].tolist())

        for dfp in [self.train_df, self.test_df]:
            seqs = self.tokenizer.texts_to_sequences(dfp["_chunk"].tolist())
            ids, lens = pad_sequences(seqs, self.max_seq_len)
            dfp["_ids"] = list(ids)
            dfp["_lens"] = list(lens)

    def train_vae(self, epochs=10, batch_size=64):
        self.model = SeqVAE(
            vocab_size=self.tokenizer.vocab_size,
            seq_len=self.max_seq_len
        ).to(self.device)

        opt = optim.Adam(self.model.parameters(), lr=1e-3)

        ids = np.stack(self.train_df["_ids"])
        lens = np.stack(self.train_df["_lens"])

        ds = SeqDataset(ids, lens)
        dl = DataLoader(ds, batch_size=batch_size, shuffle=True)

        self.model.train()
        for _ in range(epochs):
            for x, l in dl:
                x, l = x.to(self.device), l.to(self.device)
                recon, mu, logvar = self.model(x, l)
                emb = self.model.emb(x)
                mask = (x != 0).unsqueeze(-1)

                recon_loss = ((recon - emb) ** 2 * mask).sum() / mask.sum()
                kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())

                loss = recon_loss + kl
                opt.zero_grad()
                loss.backward()
                opt.step()

    def _encode(self, dfp):
        self.model.eval()
        ids = np.stack(dfp["_ids"])
        lens = np.stack(dfp["_lens"])
        ds = SeqDataset(ids, lens)
        dl = DataLoader(ds, batch_size=128)

        zs = []
        with torch.no_grad():
            for x, l in dl:
                x, l = x.to(self.device), l.to(self.device)
                mu, _ = self.model.encode(x, l)
                zs.append(mu.cpu().numpy())

        return np.vstack(zs)

    def train_classifier(self):
        X_train = self._encode(self.train_df)
        y_train = self.train_df[self.label_col].values

        X_test = self._encode(self.test_df)
        y_test = self.test_df[self.label_col].values

        self.classifier = LogisticRegression(max_iter=500)
        self.classifier.fit(X_train, y_train)

        return {
            "train_accuracy": accuracy_score(y_train, self.classifier.predict(X_train)),
            "test_accuracy": accuracy_score(y_test, self.classifier.predict(X_test))
        }

    def predict_text(self, text: str) -> Dict[str, Any]:
        clean = keep_english(text)
        chunk = self.tfidf.best_window(clean)

        seq = self.tokenizer.texts_to_sequences([chunk])
        ids, lens = pad_sequences(seq, self.max_seq_len)

        self.model.eval()
        with torch.no_grad():
            x = torch.tensor(ids).to(self.device)
            l = torch.tensor(lens).to(self.device)
            mu, _ = self.model.encode(x, l)
            pred = self.classifier.predict(mu.cpu().numpy())[0]
            prob = self.classifier.predict_proba(mu.cpu().numpy())[0].max()

        return {
            "selected_text": chunk,
            "predicted_label": pred,
            "confidence": prob
        }


# =========================
# End of file
# =========================
